import numpy as np
import copy
from scipy.special import logit, expit


def llr_from_bins(mated_scores, non_mated_scores, num_bins=0):
    """Estimation of Log-likelihood ratios (LRR) using
    descretized bins.

    It computes  log(P[s|mated]/P[s|non-mated])
    by estimating P[s|mated] using histograms


    Parameters
    ----------
    mated_scores : Array_like
        List of scores associated to mated pairs
    non_mated_scores : Array_like
        List of scores associated to non-mated pairs
    num_bins : Int, Optional
        Number of Bins, default automated

    Returns
    -------
    mated_llrs : ndarray
        LRRs associated to the input mated scores.
    nonmated_llrs : ndarray
        LRRs associated to the input non-mated scores.
    """
    if num_bins == 0 :
      # Limiting the number of bins
      #(100 maximum or lower if few scores available)
    	num_bins = min(int(len(mated_scores) / 2), 100)
    # Generating the bins
    maxS=max([max(mated_scores), max(non_mated_scores)])
    minS=min([min(mated_scores), min(non_mated_scores)])
    bin_edges=np.linspace(minS, maxS, num=num_bins + 1, endpoint=True)
    # Estimating P[s|mated] and P[s|non-mated]
    y1 = np.histogram(mated_scores, bins = bin_edges, density = True)[0]
    y2 = np.histogram(non_mated_scores, bins = bin_edges, density = True)[0]
    # LR = P[s|mated ]/P[s|non-mated]
    LR = np.divide(y1, y2, out=np.ones_like(y1), where=y2!=0)
    LLR = np.log(LR)
    # Function to extract the index of the correct bin for a score
    def firstGreaterIndex(tab,value):
        return next(x[0] for x in enumerate(tab) if x[1] > value)
    # Associated each score to the LLR of its bin
    mated_llrs= [LLR[firstGreaterIndex(bin_edges,s)-1] if s!=maxS else len(bin_edges)-1 for s in mated_scores]
    nonmated_llrs= [LLR[firstGreaterIndex(bin_edges,s)-1] if s!=maxS else len(bin_edges)-1 for s in non_mated_scores]
    return np.array(mated_llrs), np.array(nonmated_llrs)


def pavx(y):
    """PAV: Pool Adjacent Violators algorithm
    With respect to an input vector v, it computes ghat
    a nondecreasing vector such as sum((y - ghat).^2) is minimal

    Parameters
    ----------
    y : ndarray
        Input vector

    Returns
    -------
    ghat : ndarray
        output array.
    width : ndarray
        Width of bins generated by the PAV
    height : ndarray
        Height of bins generated by the PAV


    Notes
    -----
    Credits to Andreas Nautsch (EURECOM)
    https://gitlab.eurecom.fr/nautsch/cllr

    References
    ----------

    .. [2] Br端mmer, N., & Du Preez, J. (2006).
    Application-independent evaluation of speaker detection.
    Computer Speech & Language, 20(2-3), 230-275.
    """
    assert y.ndim == 1, 'Argument should be a 1-D array'
    assert y.shape[0] > 0, 'Input array is empty'
    n = y.shape[0]

    index = np.zeros(n, dtype=int)
    length = np.zeros(n, dtype=int)

    ghat = np.zeros(n)

    ci = 0
    index[ci] = 1
    length[ci] = 1
    ghat[ci] = y[0]

    for j in range(1, n):
        ci += 1
        index[ci] = j + 1
        length[ci] = 1
        ghat[ci] = y[j]
        while (ci >= 1) & (ghat[np.max(ci - 1, 0)] >= ghat[ci]):
            nw = length[ci - 1] + length[ci]
            ghat[ci - 1] = ghat[ci - 1] + (length[ci] / nw) * (ghat[ci] - ghat[ci - 1])
            length[ci - 1] = nw
            ci -= 1

    height = copy.deepcopy(ghat[:ci + 1])
    width = copy.deepcopy(length[:ci + 1])

    while n >= 0:
        for j in range(index[ci], n + 1):
            ghat[j - 1] = ghat[ci]
        n = index[ci] - 1
        ci -= 1

    return ghat, width, height


def optimal_llr(tar, non, laplace=False, monotonicity_epsilon=1e-6, compute_eer=False):
    """Uses PAV algorithm to optimally calibrate the score

    Parameters
    ----------
    tar : ndarray
        list of scores associated to target pairs
        (uncalibrated LLRs)
    non : ndarray
        list of scores associated to non-target pairs
        (uncalibrated LLRs)
    laplace : bool
        Use Laplace technique to avoid infinite values of LLRs
    monotonicity_epsilon : float
        Unsures monoticity of the optimal LLRs
    compute_eer : bool
        Returns ROCCH-EER

    Returns
    -------
    tar : ndarray
        Target optimally calibrated scores (PAV)
    non : ndarray
        Non-target optimally calibrated scores (PAV)
    eer : float
        ROCCH-EER

    Notes
    -----
    Credits to Andreas Nautsch (EURECOM)
    https://gitlab.eurecom.fr/nautsch/cllr

    References
    ----------

    .. [2] Br端mmer, N., & Du Preez, J. (2006).
    Application-independent evaluation of speaker detection.
    Computer Speech & Language, 20(2-3), 230-275.
    """
    scores = np.concatenate([non, tar])
    Pideal = np.concatenate([np.zeros(len(non)), np.ones(len(tar))])

    perturb = np.argsort(scores, kind='mergesort')
    Pideal = Pideal[perturb]

    if laplace:
        Pideal = np.hstack([1, 0, Pideal, 1, 0])

    Popt, width, foo = pavx(Pideal)

    if laplace:
        Popt = Popt[2:len(Popt) - 2]

    posterior_log_odds = logit(Popt)
    log_prior_odds = np.log(len(tar) / len(non))
    llrs = posterior_log_odds - log_prior_odds
    N = len(tar) + len(non)
    llrs = llrs + np.arange(N) * monotonicity_epsilon / N  # preserve monotonicity

    idx_reverse = np.zeros(len(scores), dtype=int)
    idx_reverse[perturb] = np.arange(len(scores))
    tar_llrs = llrs[idx_reverse][len(non):]
    nontar_llrs = llrs[idx_reverse][:len(non)]

    if not compute_eer:
        return tar_llrs, nontar_llrs

    nbins = width.shape[0]
    pmiss = np.zeros(nbins + 1)
    pfa = np.zeros(nbins + 1)
    #
    # threshold leftmost: accept everything, miss nothing
    left = 0  # 0 scores to left of threshold
    fa = non.shape[0]
    miss = 0
    #
    for i in range(nbins):
        pmiss[i] = miss / len(tar)
        pfa[i] = fa /len(non)
        left = int(left + width[i])
        miss = np.sum(Pideal[:left])
        fa = len(tar) + len(non) - left - np.sum(Pideal[left:])
    #
    pmiss[nbins] = miss / len(tar)
    pfa[nbins] = fa / len(non)

    eer = 0
    for i in range(pfa.shape[0] - 1):
        xx = pfa[i:i + 2]
        yy = pmiss[i:i + 2]

        # xx and yy should be sorted:
        assert (xx[1] <= xx[0]) & (yy[0] <= yy[1]), \
            'pmiss and pfa have to be sorted'

        XY = np.column_stack((xx, yy))
        dd = np.dot(np.array([1, -1]), XY)
        if np.min(np.abs(dd)) == 0:
            eerseg = 0
        else:
            # find line coefficients seg s.t. seg'[xx(i);yy(i)] = 1,
            # when xx(i),yy(i) is on the line.
            seg = np.linalg.solve(XY, np.array([[1], [1]]))
            # candidate for EER, eer is highest candidate
            eerseg = 1 / (np.sum(seg))

        eer = max([eer, eerseg])
    return tar_llrs, nontar_llrs, eer


def bayes_error_rate(mated_scores, non_mated_scores, prior_log_odds):
    """Computes the bayes error rate corresponding to the score for different prior log-odds

    prior  P1 = Pr(Hm) (probability of mated or target)
    prior_log_odds = lambda = logit(P1)
    proportion of misses: Pmiss
    proportion of false alarms: Pfa
    returns Pe = P1 Pmiss(-lambda) + (1-P1) Pfa(-lambda)
    Pe is a vector of len(prior_log_odds), i.e., one Pe per prior
    ----------
    matedScores : Array_like
        list of scores associated to mated pairs
    nonMatedScores : Array_like
        list of scores associated to non-mated pairs
    prior_log_odds : Array_like
        list of prior log-odds corresponding to each output


    Returns
    -------
    pe : ndarray
        Bayes error rate per input prior log-odds

    Notes
    -----
    This code was inspired by some function of the BOSARIS
    toolkit by Niko Br端mmer and Edward de Villiers
    https://sites.google.com/site/bosaristoolkit/

    References
    ----------

    .. [2] Br端mmer, N., & Du Preez, J. (2006).
    Application-independent evaluation of speaker detection.
    Computer Speech & Language, 20(2-3), 230-275.
    """
    pmiss = np.zeros(len(prior_log_odds))
    pfa = np.zeros(len(prior_log_odds))
    pe = np.zeros(len(prior_log_odds))
    for i, v_prior_log_odds in enumerate(prior_log_odds):
        # mated or target trials
        posteriors = np.array([expit(llr + v_prior_log_odds) for llr in mated_scores])
        pmiss[i] = np.mean((1 - np.sign(posteriors - 0.5)) / 2)
        # non mated or non target trials
        posteriors = np.array([expit(llr + v_prior_log_odds) for llr in non_mated_scores])
        pfa[i] = np.mean((1 - np.sign(0.5 - posteriors)) / 2)
        # Pe = P1 Pmiss(-lambda) + (1-P1) Pfa(-lambda)
        pe[i] = pmiss[i] * expit(v_prior_log_odds) + pfa[i] * expit(-v_prior_log_odds)
    return pe


def rocch_pava(tar_scores, nontar_scores, laplace=False):
    """ROCCH: ROC Convex Hull.
    Note: pmiss and pfa contain the coordinates of the vertices of the
    ROC Convex Hull.

    :param tar_scores: vector of target scores
    :param nontar_scores: vector of classB_scores-target scores

    :return: a tupple of two vectors: Pmiss, Pfa
    """
    Nt = tar_scores.shape[0]
    Nn = nontar_scores.shape[0]
    N = Nt + Nn
    scores = np.concatenate((tar_scores, nontar_scores))
    # Pideal is the ideal, but classB_scores-monotonic posterior
    Pideal = np.concatenate((np.ones(Nt), np.zeros(Nn)))

    # It is important here that scores that are the same
    # (i.e. already in order) should NOT be swapped.rb
    perturb = np.argsort(scores, kind='mergesort')
    #
    Pideal = Pideal[perturb]

    if laplace:
       Pideal = np.hstack([1,0,Pideal,1,0])

    Popt, width, foo = pavx(Pideal)

    if laplace:
      Popt = Popt[2:len(Popt)-2]

    nbins = width.shape[0]
    pmiss = np.zeros(nbins + 1)
    pfa = np.zeros(nbins + 1)

    # threshold leftmost: accept everything, miss nothing
    left = 0  # 0 scores to left of threshold
    fa = Nn
    miss = 0

    for i in range(nbins):
        pmiss[i] = miss / Nt
        pfa[i] = fa / Nn
        left = int(left + width[i])
        miss = Pideal[:left].sum()
        fa = N - left - Pideal[left:].sum()

    pmiss[nbins] = miss / Nt
    pfa[nbins] = fa / Nn

    return pmiss, pfa, Popt, perturb


def optimal_llr_from_Popt(Popt, perturb, Ntar, Nnon, monotonicity_epsilon=1e-6):
    posterior_log_odds = logit(Popt)
    log_prior_odds = np.log(Ntar/Nnon)
    llrs = posterior_log_odds - log_prior_odds
    N = Ntar + Nnon
    llrs = llrs + np.arange(N) * monotonicity_epsilon/N # preserve monotonicity

    idx_reverse = np.zeros(N, dtype=int)
    idx_reverse[perturb] = np.arange(N)
    llrs_reverse = llrs[idx_reverse]
    tar_llrs = llrs_reverse[:Ntar]
    nontar_llrs = llrs_reverse[Ntar:]

    return tar_llrs, nontar_llrs
